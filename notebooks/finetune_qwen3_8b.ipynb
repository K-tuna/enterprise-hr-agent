{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## ğŸ¯ ì´ ë…¸íŠ¸ë¶ì´ í•˜ëŠ” ì¼\n\n### íŒŒì¸íŠœë‹(Fine-tuning)ì´ë€?\nì´ë¯¸ í•™ìŠµëœ AI ëª¨ë¸ì„ **ë‚´ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ**ì‹œì¼œ íŠ¹ì • ë¶„ì•¼ì— ë” ì˜í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ\n\n**ë¹„ìœ **: \n- ì˜ëŒ€ ì¡¸ì—…ìƒ (ê¸°ë³¸ ëª¨ë¸) â†’ íŠ¹ì • ë³‘ì›ì—ì„œ ìˆ˜ë ¨ (íŒŒì¸íŠœë‹) â†’ ê·¸ ë³‘ì› ì „ë¬¸ì˜ (íŠ¹í™” ëª¨ë¸)\n- ì˜ì–´ ì„ ìƒë‹˜ (ê¸°ë³¸ ëª¨ë¸) â†’ ë²•ë¥  ë¬¸ì„œë¡œ ì¶”ê°€ í•™ìŠµ (íŒŒì¸íŠœë‹) â†’ ë²•ë¥  ì˜ì–´ ì „ë¬¸ê°€ (íŠ¹í™” ëª¨ë¸)\n\n### ìš°ë¦¬ì˜ ëª©í‘œ\n| Before | After |\n|--------|-------|\n| qwen3:8b (ë²”ìš© í•œêµ­ì–´ ëª¨ë¸) | qwen3-hr (HR ì „ë¬¸ê°€ ëª¨ë¸) |\n| \"ê°œë°œíŒ€ ê¸‰ì—¬\" â†’ ê°€ë” í‹€ë¦° SQL | \"ê°œë°œíŒ€ ê¸‰ì—¬\" â†’ ì •í™•í•œ SQL |\n| íšŒì‚¬ ê·œì • ëª¨ë¦„ | íšŒì‚¬ ê·œì • í•™ìŠµë¨ |\n\n### ì „ì²´ ê³¼ì • (ì•½ 40ë¶„)\n```\n1. í™˜ê²½ ì„¤ì • (5ë¶„) - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n2. ë°ì´í„° ë¡œë“œ (1ë¶„) - í•™ìŠµìš© ë°ì´í„° 200ê°œ ë¶ˆëŸ¬ì˜¤ê¸°\n3. ëª¨ë¸ ë¡œë“œ (5ë¶„) - Qwen3-8B ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ\n4. í•™ìŠµ (20-30ë¶„) - ì‹¤ì œ íŒŒì¸íŠœë‹\n5. GGUF ë³€í™˜ (5ë¶„) - Ollamaì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n6. ì €ì¥ (1ë¶„) - Google Driveì— ì €ì¥\n```\n\n### ì‹œì‘í•˜ê¸° ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸\n- [ ] Google ê³„ì • ë¡œê·¸ì¸\n- [ ] ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ **T4 GPU** ì„ íƒ\n- [ ] combined_train.json íŒŒì¼ ì¤€ë¹„ (Google Drive ë˜ëŠ” ë¡œì»¬)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-8B HR ë„ë©”ì¸ íŒŒì¸íŠœë‹\n",
    "\n",
    "**í™˜ê²½**: Google Colab (T4 15GB / A100 40GB)\n",
    "\n",
    "**ëª©í‘œ**:\n",
    "- SQL ìƒì„± í’ˆì§ˆ í–¥ìƒ\n",
    "- íšŒì‚¬ ê·œì • QA í’ˆì§ˆ í–¥ìƒ\n",
    "\n",
    "**ë°ì´í„°ì…‹**: 200ê°œ (SQL 100 + RAG 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n# ============================================================\n# unsloth: íŒŒì¸íŠœë‹ì„ 2ë°° ë¹ ë¥´ê²Œ, VRAM 70% ì ˆì•½í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n# trl: Transformer Reinforcement Learning - í•™ìŠµ ë„êµ¬\n# peft: Parameter-Efficient Fine-Tuning - LoRA ë“± íš¨ìœ¨ì  í•™ìŠµ ê¸°ë²•\n# accelerate: GPU ê°€ì†í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n# bitsandbytes: 4-bit/8-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n#\n# %%capture: ì„¤ì¹˜ ë¡œê·¸ ìˆ¨ê¸°ê¸° (ê¹”ë”í•œ ì¶œë ¥)\n# --no-deps: ì˜ì¡´ì„± ì¶©ëŒ ë°©ì§€\n# ============================================================\n\n%%capture\n!pip install unsloth\n!pip install --no-deps trl peft accelerate bitsandbytes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ ì—…ë¡œë“œ\n",
    "\n",
    "GitHubì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ íŒŒì¼ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: GitHub Raw URLì—ì„œ ë‹¤ìš´ë¡œë“œ (repoê°€ publicì¸ ê²½ìš°)\n",
    "# !wget -O combined_train.json \"https://raw.githubusercontent.com/K-tuna/enterprise-hr-agent/main/data/finetuning/combined_train.json\"\n",
    "\n",
    "# ë°©ë²• 2: Google Driveì—ì„œ ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Driveì— combined_train.json ì—…ë¡œë“œ í›„ ê²½ë¡œ ì§€ì •\n",
    "DATASET_PATH = \"/content/drive/MyDrive/enterprise-hr-agent/combined_train.json\"\n",
    "\n",
    "# ë°©ë²• 3: ì§ì ‘ ì—…ë¡œë“œ\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # combined_train.json ì„ íƒ\n",
    "# DATASET_PATH = \"combined_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í™•ì¸\n",
    "import json\n",
    "\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"ì´ ë°ì´í„° ìˆ˜: {len(raw_data)}ê°œ\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ì˜ˆì‹œ:\")\n",
    "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ¤– ëª¨ë¸ ë¡œë“œ\n# ============================================================\n# FastLanguageModel.from_pretrained(): ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ\n#\n# íŒŒë¼ë¯¸í„° ì„¤ëª…:\n# - model_name: ì‚¬ìš©í•  ëª¨ë¸ ID\n#   * \"unsloth/Qwen3-8B\" = Qwen3 8B íŒŒë¼ë¯¸í„° ë²„ì „ (Unsloth ìµœì í™”)\n#   * 8B = 80ì–µ ê°œì˜ íŒŒë¼ë¯¸í„° (ê°€ì¤‘ì¹˜)\n#\n# - max_seq_length: ìµœëŒ€ ì…ë ¥ ê¸¸ì´\n#   * 2048 í† í° â‰ˆ í•œê¸€ ì•½ 1000ì\n#   * ê¸¸ìˆ˜ë¡ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ê°€ëŠ¥, ë©”ëª¨ë¦¬ ë” ì‚¬ìš©\n#\n# - load_in_4bit: 4ë¹„íŠ¸ ì–‘ìí™”\n#   * True: 32ë¹„íŠ¸ â†’ 4ë¹„íŠ¸ë¡œ ì••ì¶• (ë©”ëª¨ë¦¬ 8ë°° ì ˆì•½!)\n#   * í’ˆì§ˆ ì†ì‹¤ ê±°ì˜ ì—†ìŒ\n#\n# - dtype: ë°ì´í„° íƒ€ì…\n#   * None = ìë™ ê°ì§€ (GPUì— ë§ê²Œ ìµœì í™”)\n#\n# ê²°ê³¼ë¬¼:\n# - model: AI ëª¨ë¸ ê°ì²´\n# - tokenizer: í…ìŠ¤íŠ¸ â†” ìˆ«ì ë³€í™˜ê¸°\n# ============================================================\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# ì„¤ì •ê°’\nmax_seq_length = 2048  # ìµœëŒ€ ì…ë ¥ ê¸¸ì´ (í† í° ë‹¨ìœ„)\nlora_rank = 32         # LoRA í–‰ë ¬ í¬ê¸° (ì•„ë˜ ì…€ì—ì„œ ì„¤ëª…)\n\n# ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen3-8B\",  # Qwen3 8B íŒŒë¼ë¯¸í„° ëª¨ë¸\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,              # 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n    dtype=None,                     # ìë™ ê°ì§€\n)\n\nprint(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.config.model_type}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ”§ LoRA ì–´ëŒ‘í„° ì„¤ì •\n# ============================================================\n# LoRA (Low-Rank Adaptation): ì „ì²´ ëª¨ë¸ì´ ì•„ë‹Œ ì¼ë¶€ë§Œ í•™ìŠµí•˜ëŠ” ê¸°ìˆ \n#\n# ì™œ LoRAë¥¼ ì‚¬ìš©í•˜ë‚˜?\n# - ì „ì²´ 8B íŒŒë¼ë¯¸í„° í•™ìŠµ â†’ GPU ë©”ëª¨ë¦¬ ìˆ˜ë°± GB í•„ìš”\n# - LoRA: 0.1% íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ â†’ 16GBë¡œ ì¶©ë¶„!\n# - ì›ë³¸ ëª¨ë¸ì€ ê·¸ëŒ€ë¡œ ë³´ì¡´ (ì•ˆì „)\n#\n# íŒŒë¼ë¯¸í„° ì„¤ëª…:\n# - r (rank): LoRA í–‰ë ¬ í¬ê¸°\n#   * 16: ê°€ë²¼ìš´ í•™ìŠµ (ë¹ ë¦„, ë©”ëª¨ë¦¬ ì ìŒ)\n#   * 32: ê¶Œì¥ (í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜•) â­\n#   * 64: ê³ í’ˆì§ˆ (ëŠë¦¼, ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”)\n#\n# - target_modules: í•™ìŠµí•  ë ˆì´ì–´ë“¤\n#   * q_proj, k_proj, v_proj: Attentionì˜ Query, Key, Value\n#   * o_proj: Attention ì¶œë ¥\n#   * gate_proj, up_proj, down_proj: FFN (Feed-Forward Network)\n#   â†’ ì´ 7ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµí•´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥!\n#\n# - lora_alpha: í•™ìŠµ ê°•ë„ (ë³´í†µ rankì˜ 2ë°°)\n# - lora_dropout: ê³¼ì í•© ë°©ì§€ (0 = ì‚¬ìš© ì•ˆ í•¨)\n# - use_gradient_checkpointing: ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ë²•\n# ============================================================\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=lora_rank,                    # LoRA rank (32 ê¶Œì¥)\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention ë ˆì´ì–´\n        \"gate_proj\", \"up_proj\", \"down_proj\",      # FFN ë ˆì´ì–´\n    ],\n    lora_alpha=lora_rank * 2,       # í•™ìŠµ ê°•ë„ (rankì˜ 2ë°°)\n    lora_dropout=0,                 # ê³¼ì í•© ë°©ì§€ (0 = ì‚¬ìš© ì•ˆ í•¨)\n    bias=\"none\",                    # biasëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ\n    use_gradient_checkpointing=\"unsloth\",  # ë©”ëª¨ë¦¬ ì ˆì•½\n    random_state=42,                # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n    max_seq_length=max_seq_length,\n)\n\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"âœ… í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„°ì…‹ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„\n# ============================================================\n# ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •\n#\n# 1. get_chat_template(): ëª¨ë¸ë³„ ëŒ€í™” í˜•ì‹ í…œí”Œë¦¿ ì ìš©\n#    - Qwen ëª¨ë¸ì€ <|im_start|>user ... <|im_end|> í˜•ì‹ ì‚¬ìš©\n#    - ëª¨ë¸ë§ˆë‹¤ í˜•ì‹ì´ ë‹¤ë¦„ (ChatGPT, Claude ë“± ê°ê° ë‹¤ë¦„)\n#\n# 2. standardize_sharegpt(): ë°ì´í„° í˜•ì‹ í‘œì¤€í™”\n#    - ì…ë ¥: {\"conversations\": [{\"role\": \"user\", ...}, ...]}\n#    - ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í˜•ì‹ì„ í†µì¼ëœ í˜•íƒœë¡œ ë³€í™˜\n#\n# 3. apply_chat_template(): ëŒ€í™”ë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n#    - ì…ë ¥: [{\"role\": \"user\", \"content\": \"ì§ˆë¬¸\"}]\n#    - ì¶œë ¥: \"<|im_start|>user\\nì§ˆë¬¸<|im_end|>\\n<|im_start|>assistant\\n\"\n#\n# ì™œ ì´ëŸ° ë³€í™˜ì´ í•„ìš”í•œê°€?\n# - ëª¨ë¸ì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ì´í•´í•¨\n# - \"userê°€ ë§í–ˆë‹¤\", \"assistantê°€ ë‹µí–ˆë‹¤\"ë¥¼ íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ êµ¬ë¶„\n# ============================================================\n\nfrom datasets import Dataset\nfrom unsloth.chat_templates import get_chat_template, standardize_sharegpt\n\n# 1. Tokenizerì— Qwenìš© chat template ì ìš©\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template=\"qwen-2.5\",  # Qwen3ë„ ë™ì¼í•œ í…œí”Œë¦¿ ì‚¬ìš©\n)\n\n# 2. í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜\ndef formatting_prompts_func(examples):\n    \"\"\"ëŒ€í™” ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n    convos = examples[\"conversations\"]\n    texts = [\n        tokenizer.apply_chat_template(\n            convo, \n            tokenize=False,              # í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (í† í° ì•„ë‹˜)\n            add_generation_prompt=False  # í•™ìŠµìš©ì´ë¯€ë¡œ ìƒì„± í”„ë¡¬í”„íŠ¸ ë¶ˆí•„ìš”\n        ) \n        for convo in convos\n    ]\n    return {\"text\": texts}\n\n# 3. Dataset ìƒì„± ë° ë³€í™˜\ndataset = Dataset.from_list(raw_data)           # ë¦¬ìŠ¤íŠ¸ â†’ Dataset ê°ì²´\ndataset = standardize_sharegpt(dataset)         # í˜•ì‹ í‘œì¤€í™”\ndataset = dataset.map(formatting_prompts_func, batched=True)  # ë³€í™˜ ì ìš©\n\n# ê²°ê³¼ í™•ì¸\nprint(f\"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ\")\nprint(f\"\\nğŸ“ í¬ë§·íŒ…ëœ ì²« ë²ˆì§¸ ì˜ˆì‹œ (ì²˜ìŒ 500ì):\")\nprint(dataset[0][\"text\"][:500])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ í•™ìŠµ ì„¤ì •\n# ============================================================\n# SFTTrainer: Supervised Fine-Tuning Trainer (ì§€ë„í•™ìŠµ íŠ¸ë ˆì´ë„ˆ)\n# - ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë„êµ¬\n#\n# ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…:\n#\n# [ë°°ì¹˜ ê´€ë ¨]\n# - per_device_train_batch_size (2): í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ìˆ˜\n#   * í´ìˆ˜ë¡ ë¹ ë¥´ì§€ë§Œ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©\n#   * ë©”ëª¨ë¦¬ ë¶€ì¡±í•˜ë©´ 1ë¡œ ì¤„ì´ê¸°\n#\n# - gradient_accumulation_steps (4): ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  íšŸìˆ˜\n#   * ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = batch_size Ã— accumulation = 2Ã—4 = 8\n#   * ë©”ëª¨ë¦¬ ë¶€ì¡±í•  ë•Œ: batch_sizeâ†“, accumulationâ†‘\n#\n# [í•™ìŠµ ê´€ë ¨]\n# - num_train_epochs (3): ì „ì²´ ë°ì´í„° ë°˜ë³µ í•™ìŠµ íšŸìˆ˜\n#   * 1: í•™ìŠµ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ\n#   * 3: ê¶Œì¥ â­\n#   * 5+: ê³¼ì í•© ìœ„í—˜ (ì™¸ì›Œë²„ë¦¼)\n#\n# - learning_rate (2e-4 = 0.0002): í•™ìŠµë¥ \n#   * í•œ ë²ˆì— ì–¼ë§ˆë‚˜ ìˆ˜ì •í• ì§€ ê²°ì •\n#   * ë„ˆë¬´ í¬ë©´: í•™ìŠµ ë¶ˆì•ˆì • (íŠ•ê¹€)\n#   * ë„ˆë¬´ ì‘ìœ¼ë©´: í•™ìŠµ ëŠë¦¼\n#   * LoRA ê¶Œì¥: 1e-4 ~ 3e-4\n#\n# - warmup_steps (10): ì›Œë°ì—… êµ¬ê°„\n#   * ì²˜ìŒì— í•™ìŠµë¥ ì„ ì²œì²œíˆ ì˜¬ë¦¼ (ì•ˆì •ì„±)\n#\n# [ë©”ëª¨ë¦¬ ì ˆì•½]\n# - fp16/bf16: ë°˜ì •ë°€ë„ í•™ìŠµ (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)\n# - optim=\"adamw_8bit\": 8ë¹„íŠ¸ ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ ì ˆì•½)\n# - packing=True: ì§§ì€ ì‹œí€€ìŠ¤ë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ íš¨ìœ¨ì„± í–¥ìƒ\n# ============================================================\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",           # í•™ìŠµí•  í…ìŠ¤íŠ¸ í•„ë“œ\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,                  # ë°ì´í„° ì²˜ë¦¬ ë³‘ë ¬í™”\n    packing=True,                        # ì§§ì€ ì‹œí€€ìŠ¤ íŒ¨í‚¹ (íš¨ìœ¨ì„±â†‘)\n    args=TrainingArguments(\n        output_dir=\"./outputs\",          # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ\n        \n        # ë°°ì¹˜ ì„¤ì •\n        per_device_train_batch_size=2,   # GPUë‹¹ ë°°ì¹˜ í¬ê¸°\n        gradient_accumulation_steps=4,   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì‹¤ì œ ë°°ì¹˜=8)\n        \n        # í•™ìŠµ ì„¤ì •\n        num_train_epochs=3,              # ì „ì²´ ë°ì´í„° 3íšŒ ë°˜ë³µ\n        learning_rate=2e-4,              # í•™ìŠµë¥  (0.0002)\n        warmup_steps=10,                 # ì›Œë°ì—… êµ¬ê°„\n        \n        # ë¡œê¹…/ì €ì¥\n        logging_steps=10,                # 10ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n        save_steps=100,                  # 100ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n        \n        # ë©”ëª¨ë¦¬ ìµœì í™”\n        fp16=not is_bfloat16_supported(),  # GPUê°€ bf16 ë¯¸ì§€ì›ì‹œ fp16 ì‚¬ìš©\n        bf16=is_bfloat16_supported(),      # bf16 ì§€ì›ì‹œ bf16 ì‚¬ìš© (ë” ì•ˆì •ì )\n        optim=\"adamw_8bit\",              # 8ë¹„íŠ¸ ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ ì ˆì•½)\n        \n        # ê¸°íƒ€\n        weight_decay=0.01,               # ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€)\n        lr_scheduler_type=\"linear\",      # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n        seed=42,                         # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n        report_to=\"none\",                # WandB ë“± ë¦¬í¬íŒ… ë¹„í™œì„±í™”\n    ),\n)\n\nprint(\"âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ! ë‹¤ìŒ ì…€ì—ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹œì‘\n",
    "print(\"=\" * 50)\n",
    "print(\"í•™ìŠµ ì‹œì‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ì´ í•™ìŠµ ì‹œê°„: {trainer_stats.metrics['train_runtime']:.2f}ì´ˆ\")\n",
    "print(f\"ìµœì¢… Loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  ëª¨ë“œ í™œì„±í™”\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "test_questions = [\n",
    "    \"ê°œë°œíŒ€ í‰ê·  ê¸‰ì—¬ëŠ”?\",\n",
    "    \"ì—°ì°¨íœ´ê°€ ë©°ì¹ ì´ì•¼?\",\n",
    "    \"ê¹€ì² ìˆ˜ í‰ê°€ ì ìˆ˜ ì•Œë ¤ì¤˜\",\n",
    "    \"ì¬íƒê·¼ë¬´ ê°€ëŠ¥í•´?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GGUF ë³€í™˜ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ’¾ GGUF ë³€í™˜\n# ============================================================\n# GGUF (GPT-Generated Unified Format)\n# - llama.cppì—ì„œ ë§Œë“  ëª¨ë¸ íŒŒì¼ í˜•ì‹\n# - Ollama, LM Studio ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¡œì»¬ ì¶”ë¡  í‘œì¤€\n#\n# ì™œ GGUFë¡œ ë³€í™˜í•˜ë‚˜?\n# - PyTorch ëª¨ë¸ â†’ Ollamaì—ì„œ ë°”ë¡œ ì‚¬ìš© ë¶ˆê°€\n# - GGUFë¡œ ë³€í™˜í•´ì•¼ ollama run ëª…ë ¹ì–´ë¡œ ì‹¤í–‰ ê°€ëŠ¥\n#\n# quantization_method ì˜µì…˜:\n# - \"q4_k_m\": 4ë¹„íŠ¸ ì–‘ìí™” (ê¶Œì¥ â­)\n#   * íŒŒì¼ í¬ê¸°: ~5GB\n#   * í’ˆì§ˆ: ì¢‹ìŒ (ì›ë³¸ ëŒ€ë¹„ 95%+)\n#   * ì†ë„: ë¹ ë¦„\n#\n# - \"q8_0\": 8ë¹„íŠ¸ ì–‘ìí™”\n#   * íŒŒì¼ í¬ê¸°: ~9GB\n#   * í’ˆì§ˆ: ë§¤ìš° ì¢‹ìŒ (ì›ë³¸ ëŒ€ë¹„ 99%+)\n#\n# - \"f16\": 16ë¹„íŠ¸ (ì–‘ìí™” ì—†ìŒ)\n#   * íŒŒì¼ í¬ê¸°: ~16GB\n#   * í’ˆì§ˆ: ì›ë³¸ ê·¸ëŒ€ë¡œ\n#\n# âš ï¸ ì£¼ì˜: Unslothê°€ íŒŒì¼ëª…ì„ ìë™ ìƒì„±í•¨ (ì˜ˆ: qwen3-8b.Q4_K_M.gguf)\n# ============================================================\n\nimport glob\n\n# GGUF ë³€í™˜\nmodel.save_pretrained_gguf(\n    \"qwen3-hr\",                      # ì €ì¥ ë””ë ‰í† ë¦¬ ì´ë¦„\n    tokenizer,\n    quantization_method=\"q4_k_m\",    # 4ë¹„íŠ¸ ì–‘ìí™” (í’ˆì§ˆ/í¬ê¸° ê· í˜•)\n)\n\n# ìƒì„±ëœ GGUF íŒŒì¼ ì°¾ê¸° (Unslothê°€ ìë™ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±)\ngguf_files = glob.glob(\"*.gguf\") + glob.glob(\"qwen3-hr/*.gguf\")\nprint(\"âœ… GGUF ë³€í™˜ ì™„ë£Œ!\")\nprint(f\"\\nğŸ“ ìƒì„±ëœ GGUF íŒŒì¼: {gguf_files}\")\n\n# ì²« ë²ˆì§¸ GGUF íŒŒì¼ì„ ë³€ìˆ˜ì— ì €ì¥ (ë‹¤ìŒ ì…€ì—ì„œ ì‚¬ìš©)\nif gguf_files:\n    GGUF_FILE = gguf_files[0]\n    print(f\"ğŸ“¦ ì‚¬ìš©í•  íŒŒì¼: {GGUF_FILE}\")\nelse:\n    print(\"âŒ GGUF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n    GGUF_FILE = None\n\n!ls -lh *.gguf 2>/dev/null || ls -lh qwen3-hr/*.gguf 2>/dev/null || echo \"íŒŒì¼ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ’¾ Google Driveì— ì €ì¥\n# ============================================================\n# ìƒì„±ëœ GGUF íŒŒì¼ì„ Google Driveë¡œ ë³µì‚¬\n# ë¡œì»¬ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Ollamaì— ë“±ë¡í•  ìˆ˜ ìˆìŒ\n# ============================================================\n\nimport shutil\nimport os\n\nSAVE_DIR = \"/content/drive/MyDrive/enterprise-hr-agent/models\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nif GGUF_FILE and os.path.exists(GGUF_FILE):\n    # GGUF íŒŒì¼ì„ qwen3-hr.ggufë¡œ ì´ë¦„ ë³€ê²½í•˜ì—¬ ë³µì‚¬\n    dest_path = os.path.join(SAVE_DIR, \"qwen3-hr.gguf\")\n    shutil.copy(GGUF_FILE, dest_path)\n    print(f\"âœ… ì €ì¥ ì™„ë£Œ!\")\n    print(f\"   ì›ë³¸: {GGUF_FILE}\")\n    print(f\"   ì €ì¥: {dest_path}\")\nelse:\n    # í´ë°±: globìœ¼ë¡œ ë‹¤ì‹œ ì°¾ê¸°\n    import glob\n    gguf_files = glob.glob(\"*.gguf\") + glob.glob(\"qwen3-hr/*.gguf\")\n    if gguf_files:\n        src = gguf_files[0]\n        dest_path = os.path.join(SAVE_DIR, \"qwen3-hr.gguf\")\n        shutil.copy(src, dest_path)\n        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {dest_path}\")\n    else:\n        print(\"âŒ GGUF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n\nprint(f\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n!ls -lh \"{SAVE_DIR}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. ë¡œì»¬ ë°°í¬ ê°€ì´ë“œ\n\n### Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œ\n1. Google Driveì—ì„œ `qwen3-hr.gguf` íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n2. í”„ë¡œì íŠ¸ì˜ `models/` ë””ë ‰í† ë¦¬ì— ì €ì¥\n\n### Ollama ë“±ë¡\n```bash\n# models ë””ë ‰í† ë¦¬ë¡œ ì´ë™\ncd models/\n\n# Modelfile ìƒì„±\ncat > Modelfile << 'EOF'\nFROM ./qwen3-hr.gguf\n\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\nPARAMETER temperature 0.1\nPARAMETER stop \"<|im_end|>\"\nEOF\n\n# Ollamaì— ë“±ë¡\nollama create qwen3-hr -f Modelfile\n\n# í…ŒìŠ¤íŠ¸\nollama run qwen3-hr \"ê°œë°œíŒ€ í‰ê·  ê¸‰ì—¬ SQL ì‘ì„±í•´ì¤˜\"\n```\n\n### ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ\n```bash\n# ê¸°ì¡´ ëª¨ë¸\nollama run qwen3:8b \"ê°œë°œíŒ€ í‰ê·  ê¸‰ì—¬ SQL ì‘ì„±í•´ì¤˜\"\n\n# íŒŒì¸íŠœë‹ëœ ëª¨ë¸\nollama run qwen3-hr \"ê°œë°œíŒ€ í‰ê·  ê¸‰ì—¬ SQL ì‘ì„±í•´ì¤˜\"\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. Google Driveì—ì„œ GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ\")\n",
    "print(\"2. ë¡œì»¬ì—ì„œ Ollamaì— ë“±ë¡\")\n",
    "print(\"3. ê¸°ì¡´ qwen3:8bì™€ ì„±ëŠ¥ ë¹„êµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ“– ìš©ì–´ ì‚¬ì „\n\n| ìš©ì–´ | ì„¤ëª… | ë¹„ìœ  |\n|------|------|------|\n| **íŒŒì¸íŠœë‹** | ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ | ì˜ëŒ€ìƒ â†’ ì „ë¬¸ì˜ ìˆ˜ë ¨ |\n| **LoRA** | ì „ì²´ê°€ ì•„ë‹Œ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ëŠ” íš¨ìœ¨ì  ê¸°ë²• | ì „ì²´ ë¦¬ëª¨ë¸ë§ ëŒ€ì‹  ì¸í…Œë¦¬ì–´ë§Œ |\n| **ì–‘ìí™”** | 32ë¹„íŠ¸ â†’ 4/8ë¹„íŠ¸ë¡œ ì••ì¶• (ë©”ëª¨ë¦¬ ì ˆì•½) | ê³ í™”ì§ˆ â†’ ì••ì¶• ë™ì˜ìƒ |\n| **í† í°** | í…ìŠ¤íŠ¸ì˜ ìµœì†Œ ë‹¨ìœ„ | ëŒ€ëµ í•œê¸€ 0.5ì, ì˜ì–´ 0.75ë‹¨ì–´ |\n| **ì—í­ (Epoch)** | ì „ì²´ ë°ì´í„°ë¥¼ 1íšŒ í•™ìŠµí•˜ëŠ” ë‹¨ìœ„ | êµê³¼ì„œ 1íšŒë… |\n| **ë°°ì¹˜ (Batch)** | í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ë°ì´í„° ë¬¶ìŒ | í•œ ë²ˆì— í‘¸ëŠ” ë¬¸ì œ ìˆ˜ |\n| **í•™ìŠµë¥ ** | í•™ìŠµ ì†ë„ (ë„ˆë¬´ í¬ë©´ ë¶ˆì•ˆì •, ì‘ìœ¼ë©´ ëŠë¦¼) | ê±¸ìŒ ë³´í­ í¬ê¸° |\n| **GGUF** | Ollama/llama.cppìš© ëª¨ë¸ íŒŒì¼ í˜•ì‹ | MP4 ê°™ì€ ë™ì˜ìƒ í¬ë§· |\n| **VRAM** | GPU ë©”ëª¨ë¦¬ | T4: 16GB, 2070 Super: 8GB |\n| **Loss** | ëª¨ë¸ì˜ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ) | ì‹œí—˜ ì˜¤ë‹µë¥  |\n| **Attention** | ì…ë ¥ì˜ ì–´ëŠ ë¶€ë¶„ì— ì§‘ì¤‘í• ì§€ ê²°ì •í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ | ë°‘ì¤„ ê¸‹ê¸° |\n| **FFN** | Feed-Forward Network, ì •ë³´ë¥¼ ë³€í™˜í•˜ëŠ” ë ˆì´ì–´ | ì •ë³´ ê°€ê³µ ê³µì¥ |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## â“ ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ì™€ í•´ê²°ë²•\n\n### 1. \"CUDA out of memory\" (GPU ë©”ëª¨ë¦¬ ë¶€ì¡±)\n**ì¦ìƒ**: í•™ìŠµ ì¤‘ ê°‘ìê¸° ë©ˆì¶”ê³  ë©”ëª¨ë¦¬ ì—ëŸ¬ ë°œìƒ\n\n**í•´ê²° ë°©ë²•**:\n```python\n# ë°©ë²• 1: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°\nper_device_train_batch_size=1  # 2 â†’ 1ë¡œ ë³€ê²½\n\n# ë°©ë²• 2: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ëŠ˜ë¦¬ê¸°\ngradient_accumulation_steps=8  # 4 â†’ 8ë¡œ ë³€ê²½\n\n# ë°©ë²• 3: ëŸ°íƒ€ì„ ì¬ì‹œì‘\n# ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘ â†’ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹¤í–‰\n```\n\n---\n\n### 2. \"No GPU available\" (GPU ì—†ìŒ)\n**ì¦ìƒ**: GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì—ëŸ¬\n\n**í•´ê²° ë°©ë²•**:\n1. ë©”ë‰´: `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½`\n2. í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: `T4 GPU` ì„ íƒ\n3. ì €ì¥ í›„ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\n\n---\n\n### 3. \"File not found: combined_train.json\"\n**ì¦ìƒ**: ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\n\n**í•´ê²° ë°©ë²•**:\n1. Google Drive ë§ˆìš´íŠ¸ í™•ì¸ (ì™¼ìª½ íŒŒì¼ íƒ­ì—ì„œ drive í´ë” í™•ì¸)\n2. `DATASET_PATH` ë³€ìˆ˜ê°€ ì‹¤ì œ íŒŒì¼ ê²½ë¡œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n3. ì§ì ‘ ì—…ë¡œë“œ ë°©ë²• ì‚¬ìš©:\n```python\nfrom google.colab import files\nuploaded = files.upload()  # íŒŒì¼ ì„ íƒ ì°½ì—ì„œ combined_train.json ì„ íƒ\nDATASET_PATH = \"combined_train.json\"\n```\n\n---\n\n### 4. í•™ìŠµ ì¤‘ ì„¸ì…˜ ëŠê¹€\n**ì¦ìƒ**: ë¸Œë¼ìš°ì € íƒ­ì„ ì˜¤ë˜ ë‘ë©´ ì—°ê²° ëŠê¹€\n\n**ì›ì¸**: Colab ì •ì±… (90ë¶„ ìœ íœ´ íƒ€ì„ì•„ì›ƒ / 12ì‹œê°„ ìµœëŒ€)\n\n**í•´ê²° ë°©ë²•**:\n- ë¸Œë¼ìš°ì € íƒ­ì„ í™œì„± ìƒíƒœë¡œ ìœ ì§€\n- í•™ìŠµ ì¤‘ì—ëŠ” ìë™ìœ¼ë¡œ í™œì„± ìƒíƒœ ìœ ì§€ë¨\n- ëŠê²¼ì„ ê²½ìš°: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ ê°€ëŠ¥\n\n---\n\n### 5. \"unsloth not found\" (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹¤íŒ¨)\n**ì¦ìƒ**: import unsloth ì—ëŸ¬\n\n**í•´ê²° ë°©ë²•**:\n```python\n# ì„¤ì¹˜ ì…€ ë‹¤ì‹œ ì‹¤í–‰\n!pip install unsloth\n!pip install --no-deps trl peft accelerate bitsandbytes\n\n# ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìš”í•  ìˆ˜ ìˆìŒ\n```\n\n---\n\n### 6. Lossê°€ ì¤„ì§€ ì•Šê±°ë‚˜ NaN ë°œìƒ\n**ì¦ìƒ**: í•™ìŠµ ì¤‘ lossê°€ ê³„ì† ê°™ê±°ë‚˜ NaNìœ¼ë¡œ í‘œì‹œ\n\n**í•´ê²° ë°©ë²•**:\n```python\n# í•™ìŠµë¥  ë‚®ì¶”ê¸°\nlearning_rate=1e-4  # 2e-4 â†’ 1e-4\n\n# ë˜ëŠ” warmup ëŠ˜ë¦¬ê¸°\nwarmup_steps=20  # 10 â†’ 20\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}